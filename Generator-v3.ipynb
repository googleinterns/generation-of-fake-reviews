{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Generator-v3.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMHeLut/5gttDUCYrQC5DLG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"LQHng3v2z4Ra","colab_type":"text"},"source":["# Generator - 3.0\n","This notebook contains the design of the generator to generate the next words of a review using LSTM and GloVe Embeddings <br>\n","Note: This notebook is for training the Generator only. The Text Generation and Evaluation is found in *Generator-v3 Generation and Evaluation notebook*. \n","<br>\n","Files used\n","Reviews from https://www.kaggle.com/yelp-dataset/yelp-dataset?select=yelp_academic_dataset_review.json\n"]},{"cell_type":"code","metadata":{"id":"mghsRyejS2AO","colab_type":"code","colab":{}},"source":["import os\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth, drive\n","from oauth2client.client import GoogleCredentials\n","\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rbg5JMy7zyWU","colab_type":"code","colab":{}},"source":["from IPython.display import HTML, display\n","def set_css():\n","  \"\"\"A function for wrapping text displayed in colab\"\"\"\n","  display(HTML('''\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  '''))\n","  \n","get_ipython().events.register('pre_run_cell', set_css)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UCcTDT-yzyY4","colab_type":"code","colab":{}},"source":["#importing the libraries\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","from keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import random\n","from sklearn.utils import  shuffle\n","import time"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CHYlVCGfzyat","colab_type":"code","colab":{}},"source":["print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l7OoNrALzydE","colab_type":"code","colab":{}},"source":["import nltk\n","from nltk.corpus import stopwords\n","nltk.download('punkt')\n","nltk.download('stopwords')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cMpLFzp0XCUs","colab_type":"code","colab":{}},"source":["#defining the constants\n","VOCAB_SIZE = 10000\n","INPUT_LEN = 29\n","EMBEDDING_DIM = 300"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cl1rxEE2KCdF","colab_type":"text"},"source":["# Loading the data"]},{"cell_type":"code","metadata":{"id":"t0blHtqgzyiC","colab_type":"code","colab":{}},"source":["#loading the dataset\n","df = pd.read_csv(\"reviews.csv\")\n","positive_review_ratings = [5]\n","positive_reviews_df = df[df.stars.isin(positive_review_ratings)].reset_index(drop=True)\n","reviews_list = positive_reviews_df[\"text\"].values.tolist()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RLwXBo03W7kD","colab_type":"text"},"source":["# Pre-processing"]},{"cell_type":"code","metadata":{"id":"n39X8m7hKH-D","colab_type":"code","colab":{}},"source":["from nltk.tokenize import sent_tokenize\n","import re\n","from string import punctuation\n","\n","punc = set(punctuation)\n","\n","full_stop_pattern = \"\\.(?=\\S)\"\n","full_stop_pattern = re.compile(full_stop_pattern)\n","\n","qmark_pattern = \"(?=\\S)\\?\"\n","qmark_pattern = re.compile(qmark_pattern)\n","\n","exclmark_pattern = re.compile(\"(?=\\S)\\!\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w-4J0qIEzyj3","colab_type":"code","colab":{}},"source":["def modify_text(text :str) -> str:\n","  \"\"\"\n","  A function to pre-process text.\n","  \n","  It removes numbers, repeated punctuations and adds a space before and after full-stop, comma and exclamation mark.\n","  Args:\n","    text: review to be modified\n","  Returns:\n","    A string with the repeated punctuations removed and a space after .,!\n","\n","  \"\"\"\n","  #removing numbers\n","  text = re.sub(\"\\d+\", \"\", text)\n","  \n","  #removing repeated punctuation marks\n","  new_text = \"\"\n","  for i in range(len(text)):\n","    \n","    #if its not a punctuation mark then add it to the new_text\n","    if text[i] not in punc:\n","      new_text+=text[i]\n","\n","    #if text[i] is a punctuation mark, then check whether the previous character is not a punctuation mark or a space\n","    elif text[i] in punc and ((text[i-1] not in punc) and (text[i-1]!= \" \")):\n","      new_text+=text[i]\n","\n","  text = new_text\n","\n","  #removing additional spaces\n","  text = re.sub(' +', ' ', text) \n","\n","  #changing it's to its\n","  text = re.sub(\"it\\'s\", \"its\", text)\n","  text = re.sub(\"It\\'s\", \"its\", text)\n","   \n","  #removing the new line character\n","  text = re.sub(\"(\\n)+\", \" \", text)\n","  \n","  #replacing common patterns\n","  text = re.sub(\"\\'ve\", \" have\", text)\n","  text = re.sub(\"don't\", \" do not\", text)\n","  text = re.sub(\"\\'t\", \" not\", text)\n","  text = re.sub(\"\\'s\", \" is\", text)\n","  text = re.sub(\"\\'m\", \" am\", text)\n","  \n","  #removing the single quotes\n","  text = re.sub(\"\\'\", \"\", text)\n","\n","  #the tokens like !,?,. are considered as separate tokens. \n","  #Hence a space is added before/after them to make the get recognized as separate tokens.\n","\n","  # adding space after the full stop\n","  text = re.sub(full_stop_pattern, \". \", text)\n","\n","  #adding a space before ?\n","  text = re.sub(qmark_pattern, \" ?\", text)\n","\n","  #adding a space before !\n","  text = re.sub(exclmark_pattern, \" !\", text)\n","  \n","  return text\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MQlUknilzymE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":37},"executionInfo":{"status":"ok","timestamp":1595181505940,"user_tz":-330,"elapsed":1028,"user":{"displayName":"Ishwarya Sivakumar","photoUrl":"","userId":"02628689146101986402"}},"outputId":"212975b3-2cf0-4b8c-b931-0e32d8d71349"},"source":["#test case\n","modify_text(\"How are you doing????? I'm fine!! This is good, I don't hate it.\")"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic":{"type":"string"},"text/plain":["'How are you doing ? I am fine ! This is good, I  do not hate it.'"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"CjrW6tDSzyqG","colab_type":"code","colab":{}},"source":["#the punctuation marks like !, ,, . are not to be removed from the text. So they are removed from the set of punctuations\n","import string\n","punc_s = string.punctuation\n","punc_s=punc_s.replace(\"!\",'')\n","punc_s=punc_s.replace(\".\",'')\n","punc_s"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dR4IpjDZSXWq","colab_type":"code","colab":{}},"source":["import string\n","from nltk.tokenize import word_tokenize\n","from string import punctuation\n","\n","#to remove punctuations\n","table = str.maketrans('', '', punc_s)\n","punctuations_set = set(punc_s)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RLl6qCmg1lV4","colab_type":"code","colab":{}},"source":["def tokenize_text(reviews_list :list) ->list:\n","  \"\"\"\n","  A function to tokenizes the review into words and removes the reviews that have less than 10 words\n","\n","  Args:\n","    reviews_list: A list of reviews\n","  Returns: \n","    A list of lists where each list corresponds to the words in the corresponding review.\n","\n","  \"\"\"\n","  \n","  cleaned_reviews = []\n","  for line in reviews_list:\n","\n","    #tokenize the sentences into words\n","    tokens = word_tokenize(line)\n","\n","    #removing the unnecessary punctuation marks\n","    stripped = [w.translate(table) for w in tokens]\n","    \n","    #choosing a word only if it is not an unnecessary punctuation\n","    words = [word for word in stripped if ((word not in punctuations_set))]\n","    \n","\n","    #taking only the reviews whose length is greater than 10.\n","    if len(words)>10:\n","      tokens = [w.lower() for w in words if len(w)>0]\n","      cleaned_reviews.append(tokens)\n","\n","\n","  return cleaned_reviews\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dgyJrMM61nE0","colab_type":"code","colab":{}},"source":["#test case\n","sample = \"The food was great!! But no AC..delicious....i\"\n","sample = modify_text(sample)\n","sample_tokenized = tokenize_text([sample])\n","print(sample_tokenized)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VXO6br6eSvLe","colab_type":"code","colab":{}},"source":["len(reviews_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T8u4xA2Y1tS-","colab_type":"code","colab":{}},"source":["#modifying the reviews in the corpus\n","for i in range(len(reviews_list)):\n","  reviews_list[i] = modify_text(reviews_list[i])\n","#tokenization\n","cleaned_reviews = tokenize_text(reviews_list)\n","print(\"Number of reviews: \", len(cleaned_reviews))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XjfJlzW41vt6","colab_type":"code","colab":{}},"source":["def convert_to_training_set(reviews_list :list, len_of_review :int) ->list:\n","  \"\"\"\n","  A function takes in a list of reviews and converts it into an array of sentences of length len_of_review\n","\n","  Args:\n","    reviews_list: list of reviews\n","    len_of_review: an integer specifying the number of words required in a review\n","  Returns:\n","    A list of lists with each list containing the required number of words.\n","  \"\"\"\n","  reviews = []\n","  \n","  length = len_of_review\n","  \n","  #iterating through the list of reviews in the reviews_list\n","  for review in reviews_list:\n","    \n","    for i in range(length, len(review)):\n","      \n","      #slicing the reviews to sequences having len_of_review words\n","      seq = review[i-length:i]\n","      reviews.append(seq)   \n","     \n","  return reviews\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t2Sz2zM31xOB","colab_type":"code","colab":{}},"source":["#test case\n","convert_to_training_set(sample_tokenized, 4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3r-ZEiTA1y6s","colab_type":"code","colab":{}},"source":["#converting all the reviews to a sequence of 10 words\n","sequences = convert_to_training_set(cleaned_reviews, 10)\n","print(\"Total number of sequences: \", len(sequences))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JGWGcD0Y10hc","colab_type":"code","colab":{}},"source":["#assingning 9 words to the input sequence and 10th word to the prediction\n","X = []\n","y = []\n","for i in range(len(sequences)):\n","  train = sequences[i][:-1]\n","  pred = sequences[i][-1]\n","\n","  X.append(train)\n","  y.append(pred)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KMzGRbH912NM","colab_type":"code","colab":{}},"source":["#splitting into training and test sets\n","from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.0001, random_state = 100, shuffle=True)\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.01, random_state = 100, shuffle=True)\n","print(\"Size of training set: \", len(X_train))\n","print(\"Size of validation set: \", len(X_val))\n","print(\"Size of test set: \", len(X_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q1SMJGBGSuX4","colab_type":"text"},"source":["If re-training the generator model, then redefine the tokenizer. <br>\n","If using the pre-trained weights, then reload the tokenizer with which the model was trained"]},{"cell_type":"code","metadata":{"id":"OerpiuSi2PPj","colab_type":"code","colab":{}},"source":["#defining the tokenizer\n","tokenizer  = tf.keras.preprocessing.text.Tokenizer(num_words = VOCAB_SIZE,lower = True, oov_token=\"<OOV>\", filters='\"#$%&\\'()*+-/:;<=>?@[\\\\]^_`{|}~' )\n","tokenizer.fit_on_texts(X_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rQLJypTx2MbN","colab_type":"code","colab":{}},"source":["#padding and tokenization of the input\n","from keras.preprocessing.sequence import pad_sequences\n","\n","X_train_seq = tokenizer.texts_to_sequences(X_train)\n","X_train_seq = np.array(pad_sequences(X_train_seq, INPUT_LEN, padding=\"pre\",truncating=\"post\"))\n","y_train_seq = tokenizer.texts_to_sequences(y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PRVKcZF3XpwE","colab_type":"code","colab":{}},"source":["X_val_seq = tokenizer.texts_to_sequences(X_val)\n","X_val_seq = np.array(pad_sequences(X_val_seq, INPUT_LEN, padding=\"pre\",truncating=\"post\"))\n","y_val_seq = tokenizer.texts_to_sequences(y_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r_ISNxSzXpoZ","colab_type":"code","colab":{}},"source":["X_test_seq = tokenizer.texts_to_sequences(X_test)\n","X_test_seq = np.array(pad_sequences(X_test_seq, INPUT_LEN, padding=\"pre\",truncating=\"post\"))\n","y_test_seq = tokenizer.texts_to_sequences(y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-YroxuUW2Zvx","colab_type":"text"},"source":["The preprocessing steps done are\n","\n","\n","1.   Removing the sequences in X_train and y_train that have OOV (out of vocab) words in them.\n","2.   Removing the sequences with repeating words.\n","\n","These sequences affected the model's performance and hence they were removed. \n","\n"]},{"cell_type":"code","metadata":{"id":"Xzq-4FAu2YiQ","colab_type":"code","colab":{}},"source":["#finding the rows in X that have OOV token and removing them\n","rows_with_oov = []\n","for i in range(len(X_train_seq)):\n","  if 1 in X_train_seq[i]:\n","    rows_with_oov.append(i)\n","print(\"Number of rows in X_train with OOV: \", len(rows_with_oov))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mFS2Cas6YJmW","colab_type":"code","colab":{}},"source":["#deleting the rows\n","X_train_seq = np.delete(X_train_seq, rows_with_oov, axis=0)\n","y_train_seq = np.delete(y_train_seq,rows_with_oov, axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pzMDWnKkYJjq","colab_type":"code","colab":{}},"source":["#finding the rows in y that have OOV token and removing them\n","y_rows_with_oov = []\n","for i in range(len(y_train_seq)):\n","  if 1 in y_train_seq[i]:\n","    y_rows_with_oov.append(i)\n","print(\"Number of rows in y_train with OOV: \", len(y_rows_with_oov))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oNKqniA9YJfd","colab_type":"code","colab":{}},"source":["#deleting the rows\n","X_train_seq = np.delete(X_train_seq, y_rows_with_oov, axis=0)\n","y_train_seq = np.delete(y_train_seq,y_rows_with_oov, axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lNwxbZup2fiw","colab_type":"code","colab":{}},"source":["def check_patterns(strings_array :list) ->list:\n","  \"\"\"\n","  A function to check and find sequences where the same word occurs consecutively after one another\n","  \n","  For example the sequence with phrase \"I had pasta and and pizza\" will be removed because and appears twice.\n","  Args:\n","    strings_list: a numpy array of the tokenized representations of the words\n","  Returns:\n","    A list of indices which have the required pattern\n","  \"\"\"\n","  \n","  ind = []\n","  for i,words_list in enumerate(strings_array):\n","    for word_index in range(1,len(words_list)):\n","      if (words_list[word_index]==words_list[word_index-1]):\n","        ind.append(i)\n","        break\n","    \n","  return ind"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oij4HDUw2ifo","colab_type":"code","colab":{}},"source":["repeated_indices_lst = check_patterns(X_train_seq)\n","print(\"Number of rows with the same word occurring consecutively: \",len(repeated_indices_lst))\n","#deleting the indices\n","X_train_seq = np.delete(X_train_seq, repeated_indices_lst, axis=0)\n","y_train_seq = np.delete(y_train_seq, repeated_indices_lst, axis=0)\n","\n","print(\"Train dataset shape: \",X_train_seq.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sGiMZ2eO2mui","colab_type":"code","colab":{}},"source":["import collections\n","\n","#exploratory data analysis for the output sequence\n","output_tokens = collections.defaultdict(int)\n","for i in y_train_seq:\n","  output_tokens[i[0]]+=1\n","\n","#most frequent outputs\n","freq_output_tokens = []\n","for key, value in output_tokens.items():\n","  if value>1000:\n","    freq_output_tokens.append(key)\n","\n","print(\"Number of unique tokens in the output: \", len(output_tokens))\n","print(\"Number of tokens in predictions occurring more than 1000 times: \",len(freq_output_tokens))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U4G4GAa-2rLc","colab_type":"text"},"source":["## Loading Word Embeddings"]},{"cell_type":"markdown","metadata":{"id":"89myAa9d203F","colab_type":"text"},"source":["### GloVe"]},{"cell_type":"code","metadata":{"id":"Pv5TDJ_C2ole","colab_type":"code","colab":{}},"source":["# load the whole embedding into memory\n","embeddings_index = dict()\n","#path to the GloVE 300-dimensional embeddings file\n","#embeddings file can be downloaded from https://nlp.stanford.edu/projects/glove/\n","\n","f = open('glove.6B.300d.txt')\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","\n","f.close()\n","print('Loaded %s word vectors.' % len(embeddings_index))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n-3Kp82z2wrb","colab_type":"code","colab":{}},"source":["# create a weight matrix for words in training docs\n","embedding_matrix = np.zeros((VOCAB_SIZE, 300))\n","for word, i in tokenizer.word_index.items():\n","  \n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None and i<VOCAB_SIZE:\n","        embedding_matrix[i] = embedding_vector"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0cAoT5_0UUPU","colab_type":"text"},"source":["GloVE embeddings was chosen over Word2Vec embeddings because Word2Vec did not have embeddings for several stop words in the corpus."]},{"cell_type":"code","metadata":{"id":"zGSsj5ia2wpI","colab_type":"code","colab":{}},"source":["words_not_in_embed = []\n","for word, token in tokenizer.word_index.items():\n","  if (token<VOCAB_SIZE) and (word not in embeddings_index):\n","    words_not_in_embed.append(word)\n","\n","print(\"Number of words not having embeddings: \",len(words_not_in_embed))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lokVLxFs5Ej0","colab_type":"text"},"source":["# Building the generator"]},{"cell_type":"code","metadata":{"id":"14J5Txk32wmg","colab_type":"code","colab":{}},"source":["tf.keras.backend.clear_session()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pvfeaK-i5PRW","colab_type":"code","colab":{}},"source":["generator_model=tf.keras.models.Sequential()\n","\n","#embedding layer\n","generator_model.add(layers.Embedding(VOCAB_SIZE,300,weights=[embedding_matrix],input_length=INPUT_LEN,trainable=False)) \n","generator_model.add(layers.BatchNormalization())\n","#LSTM layer\n","generator_model.add(layers.Bidirectional(layers.LSTM(256,return_sequences=True)))\n","generator_model.add(layers.Dropout(0.25))\n","\n","#LSTM layer\n","generator_model.add(layers.Bidirectional(layers.LSTM(128, return_sequences=False)))\n","generator_model.add(layers.Dropout(0.25))\n","\n","#Dense layers \n","generator_model.add(layers.Dense(128)) \n","generator_model.add(layers.Dense(512)) \n","generator_model.add(layers.Dense(VOCAB_SIZE,activation='softmax')) \n","\n","#Print summary of model\n","print(generator_model.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rKhoSfcA5voj","colab_type":"code","colab":{}},"source":["#loading the pre-trained weights\n","generator_model.load_weights(\"model_weights/generator_3.keras\")\n","generator_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0015),\n","             loss='sparse_categorical_crossentropy',\n","            metrics=['accuracy'])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"znse22guaIg5","colab_type":"code","colab":{}},"source":["# Load the extension and start TensorBoard\n","\n","%load_ext tensorboard\n","%tensorboard --logdir logs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZRVlz-hLaMZH","colab_type":"code","colab":{}},"source":["from keras.callbacks import TensorBoard\n","from time import time\n","tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N0BSyUeoyN8b","colab_type":"code","colab":{}},"source":["save_checkpoint = tf.keras.callbacks.ModelCheckpoint(\"text_gen_LSTM.keras\"\n","    , monitor='val_loss', verbose=0, save_best_only=True,\n","    save_weights_only=True, mode='auto', save_freq='epoch'\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"maT3bQ1Ubd-_","colab_type":"code","colab":{}},"source":["#converting y_val_seq to numpy array\n","y_val_seq = np.array(y_val_seq)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_Kqo4mC4P0Ze","colab_type":"text"},"source":["the model was trained for nearly 100 epochs to achieve good results. Though the accuracy remains at 30% the quality of sentences are good."]},{"cell_type":"code","metadata":{"id":"X6D_NlmPaOmf","colab_type":"code","colab":{}},"source":["history = generator_model.fit(X_train_seq, y_train_seq,\n","                    epochs=100,\n","                    verbose=1,\n","                    batch_size = 2048,\n","                    validation_data=(X_val_seq, y_val_seq),\n","                    callbacks=[tensorboard, save_checkpoint]\n","              )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4ogcyv4XaZ1H","colab_type":"code","colab":{}},"source":["generator_model.save_weights(\"model_weights/generator_3.keras\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DpDBTAgNQUDb","colab_type":"text"},"source":["the texts generated by this model are available in the Generator_v3 Generation and Evaluation notebook"]}]}